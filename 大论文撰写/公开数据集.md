# 公开数据集介绍
1. MCF(Multiple Cameras Fall dataset) [uvinet E, Rougier C, Meunier J, St-Arnaud A, Rousseau J (2010) Multiple cameras fall dataset.DIRO-Universite de Montr ´ eal, Tech. Rep., 1350]
2. URFD(UR Fall Detection dataset) [Kepski M, Kwolek B (2015) Embedded system for fall detection using body-worn accelerometer and depth sensor. In: 2015 IEEE 8th International conference on intelligent data acquisition and advanced computing systems: technology and applications (IDAACS), vol 2, pp 755–759. IEEE]是由安装在不同视点的室外摄像机拍摄的人类活动序列的集合，包括30次跌倒和40个日常生活活动的视频片段。在本文中，我们只使用了30个跌倒序列和30个正常活动序列的RGB图像。我们使用OpenPose算法提取这对数据集的骨架。该数据集包含 70 个 (30 个跌倒 + 40 个日常生活活动) 序列。使用 2 台 Microsoft Kinect 相机和相应的加速度计数据记录跌倒事件。ADL 事件仅用一台设备 (camera 0) 和加速度计记录。使用 PS Move (60Hz) 和 x-IMU (256Hz) 设备收集传感器数据。
3. KARD(Kinect Activity Recognition dataset) [Gaglio S, Re GL, Morana M (2014) Human activity recognition process using 3-d posture data. IEEE Trans Human-Mach Syst 45(5):586–597]
4. NUCLA(Northwestern UCLA Action dataset) [Wang WJ, Chang JW, Haung SF, Wang RJ (2016) Human posture recognition based on images captured by the kinect sensor. Int J Adv Robot Syst 13(2):54]
5. NTU数据集：由3台Kinect v2摄像机同时拍摄。它包含60种类型的动作，通常也称为NTU 60。在这个数据集中，所提出的方法通常使用两种协议进行评估：交叉主题（X-sub）和交叉视图（X-view）。X-sub包含40320个训练样本和16560个测试样本，划分规则基于40个对象，表明训练和测试集中的行为来自不同的参与者。X视图将摄像机2和3采集的样本作为训练集（37920个样本），摄像机1采集的样本为测试集（18960个样本）。特定节点的名称和数量彼此对应，例如，3个代表颈部，16个代表左脚，25个代表右手拇指，等等。我们从NTU收集了一些跌倒数据集，并选择了跌倒行为和类似跌倒行为作为样本数据，分别包含948项和4506项，包括跌倒、捡起、坐下、穿鞋、脱鞋和跌倒。
6. 数据集UCF101：最著名的动作识别数据集，包含13320个视频，分为101类。每节课包含100到200个视频。一些主要的数据集动作包括人与人的交互、人与物的交互、乐器演奏等。此数据集包含具有不同照明变化、可变闪电条件、姿势外观和不同视点的捕获视频。
7. HMDB51数据集:HMDB51数据集是另一个众所周知的数据集，包含来自各种来源的各种动作视频，如电影、谷歌和Youtube视频等。该数据集包含6849个剪辑，分为51类。每节课至少包含101个视频片段。一些动作类别包括面部微笑、物体与面部互动、身体动作（如跳跃、站立或坐下等）。数据集中出现的主要挑战是，视频剪辑是从不同的视角拍摄的，不同的照明会随着4–6个不同受试者的不同姿势而变化。
8.  KTH数据集：2004年创建的室内和室外数据集，分辨率160×120px。特点：单视图，不现实，在受控环境中打破记录，通过改变照明条件和服装，不现实的性能分析，增加了复杂性。包含行为：散步、慢跑、跑步、拳击、握手、触摸
9.  CAVIAR数据集：2003年创建的含室内数据集，分辨率394×288px。特点：现实的性能分析在法国格勒诺布尔INRIA实验室和里斯本购物中心的入口大厅创下了纪录。包含行为：见面、散步、在商店里、浏览、摔倒、独自拿东西、打架、卖玻璃和离开商店。
10. IXMAS数据集：2006年创建，包含室内数据集，分辨率320×240px。特点：多视图–5个摄像机记录，摄像机，受控（室内/室内）条件，真实。包含行为：刮胡子、坐着、站起来、扔东西、旋转、行走、摇晃、拳打脚踢、指指点点、捡东西、什么都没有、检查手表
11. K3Da数据集：2015年创建的室内视频，分辨率256×256px。特点：从18至81岁的年轻人和老年人中收集的动议。包含行为：由参与者进行一些标准测试，如短时间身体机能、适时步行、垂直跳跃、平衡评估
12. HOHA数据集：2008年创建的室内和室外数据，分辨率240 lines。主要是聚焦图像和频繁的背景变化。
13. TUM数据集：2009年创建的室内数据，分辨率780×582px。数据的应用主要在人体运动跟踪、运动共享和活动检测领域。
14. ETISEO数据集：2020年创建的室内数据，分辨率640×480px。创建视频监控是为了提高算法的性能并减少算法与其使用条件之间的依赖性。
15. UCF YouTobe数据集：2009年创建的室内和室外数据集，分辨率240×320px。包括相机移动、视差以及对象外观、比例和照明条件变化的视频。
16. HOLLYWOOD数据集：2008年创建的室内和室外数据集，分辨率300×200px。包含32个视频的视频序列，其中包含八种动作的注释。
17. Le2i数据集：我们的视频序列包含可变的照明，以及典型的困难，如遮挡或杂乱和有纹理的背景。演员们进行各种正常的日常活动和摔倒。数据集包含191个视频，出于评估目的，我们对这些视频进行了注释，并添加了表示图像序列中坠落位置地面真实情况的额外信息。然后，对每个视频的每个帧进行注释：使用边界框手动定义身体的定位。该注释允许独立于自动身体检测来评估分类特征。
通常，专门用于跌倒检测的少数可用数据集使用相同的位置进行测试和训练。因此，它不能评估该方法对训练和测试之间的位置变化的鲁棒性。为了评估这种鲁棒性，我们从不同位置录制了数据集的视频，允许定义几个评估协议（«家»、«咖啡厅»、«办公室»和«演讲厅»）。
18. Fall Detection Dataset(FDD): 该数据集总共包含21499张图像，由5名参与者进行录像，其中包括三名女性以及两名男性，同时所有集合都增添了原始图像和水平翻转图像，以增加数据集。数据集由一台摄像机所记录，位于房顶位置，高约2.4米，且数据集包含深度图像数据以及RGB图像，同时每张图片都有相应的标签。参与者的所有活动都代表 5 种不同的姿势，这些姿势包括站立、坐着、躺着、弯曲和爬行。每个图像中只有一个参与者。
19. CDnet 2014数据集: 数据集组成、特点、用途 该数据集包含多种场景的监控视频，具有groud-touch验证数据，该数据用于对比实验数据以验证模型可以在不同室内复杂场景下进行动态前景目标检测。